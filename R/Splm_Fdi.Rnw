\documentclass{article}

\begin{document}

https://drive.google.com/file/d/1Oe__q_SkjiQkz33lqwD4gSUQ74B5GO_T/view
<<>>=
library(WDI)
library(tidyr)
library(dplyr)
library(readxl)
library(rgdal)
library(spdep)
library(GISTools)
library(countrycode)
library(data.table)
library(mice)
library(splm)
library(plm)
library(tseries)
library(gdata)

ihs <- function(x) {
    y <- log(x + sqrt(x ^ 2 + 1))
    return(y)
}# alternative Box cox transform


getwd()
setwd("..")

data_exp<-readRDS("./data/FDI_data_large.rds")
W.list.inv<-readRDS("./data/W.list.inv.rds")
W.dis<-readRDS("./data/W.dis.rds")



names(data_exp)[names(data_exp) %in% c("Government Stability","Socioeconomic Conditions","Investment Profile","Internal Conflict","External Conflict","Corruption","Military in Politics","Religious Tensions","Law and Order","Ethnic Tensions","Democratic Accountability","Bureaucracy Quality")] = c("Govers","Socio","Invest","InterC","ExterC","Corruption","MilitaryPo","Religious","LawandOrder","EthnicTensions","DemocraticAccountability","BureaucracyQuality")

#--------------------------------------------sum up pol, fin und eco

attach(data_exp)

data_exp$pol<-Govers+Socio+Invest+InterC+ExterC+Corruption+MilitaryPo+Religious+LawandOrder+EthnicTensions+DemocraticAccountability+BureaucracyQuality

data_exp$fin<-CAXGS+DebtServ+ForDebt+IntLiq+XRStab

data_exp$eco<-BudBal+CACC+Inflation

detach(data_exp)

#----------------------------------------------take first difference------------------------------------

data_exp<-pdata.frame(data_exp)

data.p <- data_exp %>%
  subset(select= c(iso2c, year, population, gdp,GDP_po)) %>%
  rename.vars(from=c("population", "gdp", "GDP_po"), to=c("pop_g", "gdp_g", "GDP_po_g"))
  

for(i in 3:ncol(data.p)){
  data.p[,i] <- diff(data.p[,i])
}

data_cty <- data_exp %>%
  left_join(data.p, by=c("iso2c", "year")) %>%
  na.omit()

remove(data.p)


# fm<-value~+gdp+log(lit)+bits#mit diffs kommen wir daweil noch zu keinem besseren Ergebnis
# cov(data_exp[,3:14])
#-----------------------------------------------------Stacking------------------------------------------------------------------

summary(data_exp$reso)


#stack it how we want it 
data_exp<-data_exp[order(data_exp$year,match(data_exp$iso2c,rownames(W.dis))),]#stack by year first
data_exp<-data_exp[order(match(data_exp$iso2c,rownames(W.dis)), data_exp$year),]#stack by Country first



##------------------------------------------------------------------------------------------------------------------------------
##----------------------------------------------------Testing-------------------------------------------------------------------
##------------------------------------------------------------------------------------------------------------------------------

colnames(data_exp)
fm<-ihs(value)~log(population)+log(gdp)+log(lit)+ log(reso)+log(GDP_po)+bits+Govers+Invest+LawandOrder+DemocraticAccountability+DebtServ+ForDebt+IntLiq+Inflation+year#as.numeric(year)+I(as.numeric(year)^2)
#fm<-ihs(value)~Within(log(population))+Within(log(gdppc))+tradec+reso+log(dist)+log(GDP_po)+bits+pol+fin+eco+lit
#fm<-ihs(value)~log(population)+log(gdp)+log(lit)+log(tradec)+ log(reso)+log(dist)+log(GDP_po)+bits+Govers+Socio+Invest+InterC+ExterC+Corruption+MilitaryPo+Religious+LawandOrder+EthnicTensions+DemocraticAccountability+BureaucracyQuality+CAXGS+DebtServ+ForDebt+IntLiq+XRStab+BudBal+CACC+Inflation+as.numeric(year)+I(as.numeric(year)^2)
#fm<-ihs(value)~Within(log(population))+Within(log(gdppc))+Within(tradec)+Within(reso)+log(dist)+Within(log(GDP_po))+bits+Within(pol)+Within(fin)+Within(eco)+Within(lit)
#fm<-ihs(value)~log(population)+log(gdp)+log(tradec)+ reso+log(dist)+log(GDP_po)+bits+pol+fin+eco+lit



#la grange multiplier test

slmer <- slmtest(fm,data = data_exp ,listw=W.list.inv, test="lme")
slmer
slmlag<- slmtest(fm,data = data_exp ,listw=W.list.inv, test="lml")
slmlag

slmler <- slmtest(fm,data = data_exp ,listw=W.list.inv, test="rlme")
slmler

slmtest(fm,data = data_exp ,listw=W.list.inv, test="rlml")

#geht nur wenn wir bits und dist weglassen
spherr <- sphtest(x=fm,data = data_exp ,listw=W.list.inv, spatial.model="sarar", method="ML")
spherr



bsktest(x=fm, data = data_exp ,listw=W.list.inv, test="LM1")
bsktest(x=fm, data = data_exp ,listw=W.list.inv, test="CLMmu")
bsktest(x=fm, data = data_exp ,listw=W.list.inv, test="CLMlambda")

bsjktest(x=fm, data = data_exp ,listw=W.list.inv, test="C.2")
bsjktest(x=fm, data = data_exp ,listw=W.list.inv, test="C.3")

sararre <- spreml(fm, data_exp ,w=W.list.inv, lag = FALSE, errors = "sr")
summary(sararre)
#how to get the impacts? 


#vlt sollten wir GDP ohne per capita nehmen


#sarsem<-spml(fm, data_exp, listw=W.list.inv, lag=TRUE, spatial.error="kkp", model="within", effect="time", method="eigen", quiet=TRUE, zero.policy=NULL, tol.solce=1e-10)#komplett schlecht

sarsem<-spml(fm, data_exp, listw=W.list.inv,model="within", effect="time", lag=TRUE, spatial.error="non")

summary(sarsem)
time <- length(unique(data_exp$year))
impacts(sararre, listw = W.list.inv, time = time)
#countrycode um iso2 zu bekommen 

#mice package impotiert datensätze für natural recources verwenden # hab daweil mal igeine lösung

#literacy variable bearbeiten 


#---------------------------------------------estimate plm Model as in the paper without lit and gdp_po

plmfm<-ihs(value)~log(population)+log(gdppc)+log(tradec)+ log(reso)+log(dist)+bits+pol+fin+eco

plm<-plm(plmfm, data_exp, model = "random")
summary(plm)







##---------------------------------------------------without zero countries---------------------------------------------------
data_exp1<-filter(data_exp, iso2c != "BF")
data_exp1<-filter(data_exp1, iso2c != "GM")

shp <- readOGR(dsn = "./data/Eurostat_RG_2016_4326_M01", layer ="CNTR_RG_01M_2016_4326", encoding = "UTF-8")

shp <- shp[shp$CNTR_ID %in% data_exp1$iso2c, ]


coords <-coordinates(shp)
distw.tot <- dnearneigh(coords,0,Inf, row.names = shp$CNTR_ID)
dnbdist.tot <- nbdists(distw.tot, coords)
gl.tot <- lapply(dnbdist.tot, function(x) 1.345756/x^2) #calculating inverse distances and take x^2 because we want to put more weight on closer neighbours
W.list.inv <- nb2listw(distw.tot, glist=gl.tot, zero.policy=FALSE, style = "W")
W.dis<-listw2mat(W.list.inv)



slmer <- slmtest(fm,data = data_exp1 ,listw=W.list.inv, test="lme")
slmer
slmlag<- slmtest(fm,data = data_exp1 ,listw=W.list.inv, test="lml")
slmlag



bsktest(x=fm, data = data_exp1 ,listw=W.list.inv, test="LM1")
bsktest(x=fm, data = data_exp1 ,listw=W.list.inv, test="CLMmu")
bsktest(x=fm, data = data_exp1 ,listw=W.list.inv, test="CLMlambda")

bsjktest(x=fm, data = data_exp1 ,listw=W.list.inv, test="C.2")
bsjktest(x=fm, data = data_exp1 ,listw=W.list.inv, test="C.3")

sararre <- spreml(fm, data_exp1 ,w=W.list.inv, lag = TRUE, errors = "sr")
summary(sararre)





###################----------------------------------------------Different W matrices--------------########
shp <- readOGR(dsn = "./data/Eurostat_RG_2016_4326_M01", layer ="CNTR_RG_01M_2016_4326", encoding = "UTF-8")

shp <- shp[shp$CNTR_ID %in% rownames(W.dis), ]


coords <-coordinates(shp)
distw.tot <- dnearneigh(coords,0,Inf, row.names = shp$CNTR_ID)
dnbdist.tot <- nbdists(distw.tot, coords)
gl.tot <- lapply(dnbdist.tot, function(x) 1.345756/x) #calculating inverse distances and take x^2 because we want to put more weight on closer neighbours
W.list.inv <- nb2listw(distw.tot, glist=gl.tot, zero.policy=FALSE, style = "W")
W.dis<-listw2mat(W.list.inv)

#selbes Ergebnis wie mit ^2 
for(i in 1:35){
print(min(dnbdist.tot[[i]]))
}#to get the minimum distance 


#-----------------------------------contiguity-------------------------------------------- 
plot(shp)
queen_nb <- poly2nb(shp, row.names = shp$CNTR_ID, queen = T)  #creates a neighborhoodlist
W.list.queen <- nb2listw(queen_nb, style = "W", zero.policy = TRUE) #creates a weights-list
W.queen <- listw2mat(W.list.queen) #creates a weigths matrix

match("MZ", rownames(W.queen))
W.queen[20,24]<-1

W.list.queen<-mat2listw(W.queen, style = "W")
W.list.inv<-W.list.queen
W.dis<-W.queen
@


Plots
<<>>=

year15<-filter(data_exp, year=="2015")
year15<-subset(year15, select = c("iso2c","value"))

shp <- merge(shp, year15, all.x = FALSE, all.y = TRUE, by.x = "CNTR_ID", by.y = "iso2c")  

#nice polt
shades <- auto.shading(shp$value)
choropleth(shp, shp$value, shades)#looks like more spatial dependencies than MI shows
choro.legend(55.98108,13.93117,  sh=shades, title="FDI Inflows from China 2015", cex = 0.5)
north.arrow(55.98108,30.13448, 1, col="red")
@




Fragen:

-in welche Richtung sollen wir unsere Arbeit entwickeln? schätzen mit pooeld und time fixed effects, schauen ob serrial correlation ein Problem ist. Verschiedene Model von den Variablen her, Daten anschauen, Pol und co aufteilen aber nicht zu stark
-bestimmen unser model mit test und AIC reicht das? so halb

-distance measured with capital cities, we just use the centroit? 
-sollen wir eco, fin und pol in die einzelteile zerlegen? ja können wir probieren, sonst in Gruppen unterteilen 
-impacts von spreml? sollte gehen

-wie haben sie das model in unserem Paper geschätzt? keine Ahnung
-wovon sollen wir die logs nehmen? nicht von ECO und co 

-stationarität? serial correlation 

-linear interpolation, geht das auch mit mice, average years of schooling for those over age 25,
reported every five years for 1960–2000? daweil nicht wichtig 
-unterschied zwischen kkp und b und wann ich was verwende?



Wichtig 
debug()
undebug
::: wenn eine externe funktion in einem Package (also für uns nicht verfügbar)


To do:
-Variablen aus eco und so rausnehmen. 
-Reso ohne GDP 
-Indices können einzeln reingegeben werden
-fixed effects sichere Seit-> within ->oder time fixed effect
-differnces schätzen, stationarität schauen und adf.test for panels
-base plm mit Spatial vergleichen
-Lybien raus geben, ohne Lybien gleich viel bessere Ergebnisse 



Problems we have run into:

-the variables had not always that impact we would have expected, that´s why we had a more detailed look what determines chinese FDI´s in difference to normal FDI´s to look for which there is a theoretical reason and for which not. 
-Serial Correlation
-Non-Staionarity
\end{document}